% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
%\usepackage{natbib}% bold math
\newcommand{\ddx}[2]{ \frac{ \partial #1}{ \partial {#2} } }
%\newcommand{\ddt}[1]{ \frac{\partial #1}{\partial t} }
\newcommand{\ddt}[1]{\partial_t #1}
\newcommand{\Grad}[1]{\nabla #1}
\newcommand{\Div}[1]{\Grad \cdot #1}
\newcommand{\Lap}[1]{\Delta #1}
\definecolor{light-gray}{gray}{0.95}

\lstdefinestyle{cxx}{ %
language=C++,
basicstyle=\footnotesize\ttfamily,  % the size of the fonts that are used for the code
% numbers=left,                   % where to put the line-numbers
% numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
% stepnumber=2,                   % the step between two line-numbers. If it's 1 each line will be numbered
% numbersep=5pt,                  % how far the line-numbers are from the code
% backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
backgroundcolor=\color{light-gray},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
commentstyle=\color{cyan},
stringstyle=\ttfamily\color{red},
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                 % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting; also try caption instead of title
% morekeywords={char,double},
escapeinside={\%*}{*)},          % if you want to add a comment within your code
% morekeywords={*,...}          % if you want to add more keywords to the set
% morekeywords={char,double}    % if you want to add more keywords to the set
frameround=fttt
}

\lstdefinestyle{fortran}{ %
language=Fortran,
basicstyle=\footnotesize\ttfamily,  % the size of the fonts that are used for the code
% numbers=left,                   % where to put the line-numbers
% numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
% stepnumber=2,                   % the step between two line-numbers. If it's 1 each line will be numbered
% numbersep=5pt,                  % how far the line-numbers are from the code
% backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
backgroundcolor=\color{light-gray},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
commentstyle=\color{cyan},
stringstyle=\ttfamily\color{red},
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                 % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting; also try caption instead of title
% morekeywords={char,double},
escapeinside={\%*}{*)},          % if you want to add a comment within your code
% morekeywords={*,...}          % if you want to add more keywords to the set
% morekeywords={char,double}    % if you want to add more keywords to the set
frameround=fttt
}

\newcommand{\kk}[1]{{\color{blue}{\it KK: #1}}}

\begin{document}

%% \title{A Sample {\ttlit ACM} SIG Proceedings Paper in LaTeX
%% Format\titlenote{(Does NOT produce the permission block, copyright information nor page numbering). For use with ACM\_PROC\_ARTICLE-SP.CLS. Supported by ACM.}}
%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}

%\conferenceinfo{EuroMPI}{2013 Madrid, Spain}

\title{
Efficient evaluation of threshold queries of derived fields in a numerical simulation database
}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor Kalin Kanov \\
     \affaddr{Department of Computer Science} \\
     \affaddr{IDIES} \\
     \affaddr{Johns Hopkins University} \\
     \affaddr{Baltimore, MD 21218} \\
     \email{kalin@cs.jhu.edu}
\alignauthor Randal Burns \\
     \affaddr{Department of Computer Science} \\
     \affaddr{IDIES} \\
     \affaddr{Johns Hopkins University}\\
     \affaddr{Baltimore, MD 21218} \\
     \email{randal@cs.jhu.edu} 
\alignauthor Cristian C. Lalescu \\
     \affaddr{Department of Applied Mathematics and Statistics} \\
     \affaddr{IDIES} \\
     \affaddr{Johns Hopkins University} \\
     \affaddr{Baltimore, MD 21218} \\
     \email{clalesc1@jhu.edu}
}
%% \and % use '\and' if you need 'another row' of author names
%% % There's nothing stopping you putting the seventh, eighth, etc.
%% % author on the opening page (as the 'third row') but we ask,
%% % for aesthetic reasons that you place these 'additional authors'
%% % in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%% \date{30 July 1999}
%% % Just remember to make sure that the TOTAL number of authors
%% % is the number that will appear on the first page PLUS the
%% % number that will appear in the \additionalauthors section.

\maketitle

\begin{abstract}
Large numerical simulations are frequently used to study many different physical phenomena. The datasets produced by these simulations are in the TB and
even PB ranges. Data-intensive architectures and compute clusters have been deployed to store, manage and provide public access to such datasets. 
The systems built on-top of these architectures, sometimes called virtual laboratories, do not always support the analysis functionality that scientists are 
interested in. In this paper we present a method for the efficient evaluation of threshold queries of derived fields for large numerical simulation datasets
stored in a cluster of relational databases. The method achieves scalability through data-parallel execution. Additionally, we introduce an application-aware 
cache for query results, which has little overhead and improves query performance by X times for queries that hit the cache.

\end{abstract}

%% % A category with the (minimum) three required fields
%\category{}{Extending Data Base Technology}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Extending Database Technology}

\keywords{databases, data-intensive computing, threshold queries, turbulence} % NOT required for Proceedings

\section{Introduction}
%What is the problem?
Better instruments, faster and bigger supercomputers and easier collaboration and sharing of data in the sciences have introduced the need
to manage increasingly large data sets. Data-intensive systems and architectures have been developed with the goal of storing and providing
fast access to such data sets. Examples include the GrayWulf and DataScope clusters \cite{Szalay, TODO} at Johns Hopkins, which have capacity 
of 1.1PB and xPB
respectively. These systems differ from the traditional HPC systems in that they aim to achieve high aggregate throughput by balancing computation
capabilities with I/O and network bandwidth. The computing systems and services developed on top of these platforms are more than pure
storage engines and usually have complex analysis routines built-in, which has largely been driven by the "move the computation to the data"
paradigm. These built-in analysis routines are most often not novel themselves. They implement core scientific functionality for the study of the
particular scientific phenomena, which was observed or simulated in the first place. The analysis routines however require novel evaluation strategies 
and methods for their execution. They have to operate on large datasets distributed across multiple nodes of a cluster of relational databases 
and in order to reduce their running times they have to make efficient use of the cluster resources. 

Finding the locations or regions of highest vorticity or those with the largest norms of the velocity or other fields of interest is
functionality that enables new insights in the study of fluid dynamics. Furthermore, threshold, top-$k$ queries and similarity search in general
are important in many different disciplines. In this paper we introduce an efficient evaluation strategy for threshold
queries over time series datasets stored in a cluster of relational databases. Our method evaluates not only threshold queries of the vector or 
scalar field data stored in the database, but also performs thresholding of derived fields.

%Why is it hard? (E.g., why do naive approaches fail?)
The evaluation of threshold queries of derived fields requires a kernel computation at each point on the grid in order to compute the field to be thresholded.
The value of the derived field at each location depends on all the values at locations within the kernel of computation. 
In order to evaluate such a query for a given time-step all of the data for that time-step must be read and each data point 
will appear in $N$ kernels of computation, where $N$ is the size of the kernel. The evaluation is thus both data and computation intensive.
It is impractical to materialize all possible derived fields due to the large size of the datasets and the limits of available storage. Therefore, threshold 
queries have to be evaluated on-demand from the stored data.
Database, operating and file system caches are effective at speeding up access to the large amounts of data stored on disk. However, as was
shown by Lopez et al. \cite{Lopez} this might not be sufficient for some interactive applications. Moreover, even if the data are available in the cache the
computation associated with the derived field still needs to be performed for each point on the grid as results of previous computations are not cached. 

%Why is it interesting and important?
Thresholding allows scientists to obtain and examine the most intense regions and features in the dataset in the case of turbulence. 
These are often the locations that have the largest vorticity norms and have intense vortices or reconnection events. 
It is important that threshold queries are evaluated in an efficient manner, because often further subsequent examination
and analysis is required to understand the physics that drive these intense events. 

%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
Data-intensive architectures and compute clusters built from commodity hardware rely on parallel I/O to multiple disks and high network bandwidth
to achieve high throughput. Such systems have only recently been deployed for the storage of large numerical simulation 
datasets. The virtual laboratories built on these systems often do not support
all of the functionality that scientists are interested in. Database systems support rollup queries, including top-$k$ queries, but in most cases these
queries apply only simple linear score functions on the attribute values of individual records. 
We present a method for the efficient evaluation of threshold queries over fields derived from the raw vector or scalar fields of the numerical simulation 
stored in the database. Our method computes and operates on derived fields, which are derived by performing kernel computations over the array data 
stored in the database.
Our method makes effective use of the cluster resources and achieves high throughput and scalability. 

%What are the key components of my approach and results? Also include any specific limitations.
We exploit the parallelism 
available in the cluster by means of data-parallel execution of the computations. Each computation is distributed across the nodes of the cluster and
is executed in parallel by several threads within a database node by means of creating multiple connections to each node. Moreover we introduce an 
application-aware cache for query results. We build on the idea of an application-aware cache introduced in \cite{Lopez}, but rather than just data 
objects we cache the actual results of the computation, which leads to query performance improvement of up to X times. In that regard the cache
that we use shares more similarities with the idea of semantic caching \cite{Ren}.

- The introduction of an application-aware cache improves the running time of queries substantially if there is a cache hit and there is 
little downside to storing query results in the cache.

- The evaluation scales with the amount of parallelism available in the cluster. 

- 3d and 4d clusters of highest vorticity locations. 

- Lays the ground work for the creation of a landmark database, which could store the location of the highest vorticity regions in the dataset.

%-The evaluation is computationally limited and embarrassingly parallel, which makes it a good candidate for a GPU implementation.

\section{Scientific Use Cases}\label{science_use_cases}
Max vorticity clustering.

\section{JHTDB}
The method that we have developed for the evaluation of threshold queries of derived fields was deployed and integrated into the 
Johns Hopkins Turbulence Databases (JHTDB). The JHTDB, built on top of the GrayWulf and DataScope clusters, serve as a public virtual laboratory for
the study of turbulent phenomena. The JHTDB store several datasets, which are the output of high-resolution numerical simulations of turbulence.
The 3d time-series data are partitioned into small sub-cubes and stored in relational databases distributed across the nodes of the cluster.
Access to the data is provided by means of Web-services and a variety of analysis functions have been
implemented and can be executed through Web-service calls.
At present the service hosts four datasets, which are available publicly. 
The data are the output of numerical simulations of forced isotropic turbulence, magnetohydrodynamics (MHD), channel 
flow turbulence and homogenous buoyancy driven turbulence.
The total amount of space occupied by the datasets is over 230 TB. 

The database nodes are part of the GrayWulf \cite{Szalay} and DataScope \cite{TODO} clusters. Each node
is running Windows Server 2008 and SQL Server 2008 R2. The data for each dataset are partitioned spatially across 4 to 8
database nodes, and each database node hosts one or more databases. We use the Morton z-order space-filling
curve to distribute the data across nodes and databases as well as to spatially subdivide each time-step
into database atoms, which are of size $8^3$. Each such atom is indexed by the time-step, which it belongs to and
by the Morton code of it's lower left corner. This combination of index and data forms a record in the database.
Queries to the data and derived fields, such as derivatives and filtered quantities are evaluated through
stored procedures or user-defined functions implemented in the Common Language Runtime (CLR) framework.

The Web-services are hosted on a front-end Web-server, which handles user requests and hosts the main Web-page portal.
The Web-server acts as a mediator sending the users' requests to the database nodes and initiating their distributed evaluation. 
It assembles the results and sends them back to the client

The JHTDB provide a variety of data-intensive analysis routines that are executed on the database nodes. These include interpolation, 
differentiation, particle tracking and spatial filtering. These tasks are often data-intensive and in order to leverage the capabilities of the cluster we
have developed data-driven batch processing techniques for their evaluation \cite{KanovSC11, KanovSC12}. What differentiates threshold and
top-$k$ queries from these tasks is the fact that they usually operate on small subsets of the space or a collection of individual target locations, whereas
threshold and top-$k$ queries usually have to examine the entire data volume. Furthermore, the data product of threshold and top-$k$ queries is much
smaller in relation to the amount of data that need to be examined. This fact combined with the fact that subsequent queries can reuse previously
computed results makes the query results suitable to caching. 

\section{Threshold Query Evaluation}

Threshold queries of derived fields submitted to the JHTDB are evaluated using a data-parallel execution strategy and the query results are 
cached in an application-aware
cache. The evaluation strategy for queries that do not hit the cache is driven by the spatial partitioning of the data across the nodes of the cluster.
In most cases threshold queries operate over an entire time-step. Therefore, each such query is subdivided by the mediator into queries submitted to
each of the database nodes. Each node evaluates the query over the data that it has stored locally. Only a small amount of data along the boundary
need to be requested from adjacent nodes. The data are read into memory and the particular field requested is computed at each of the locations on
the grid. The norm or absolute value of the derived field at each location is compared against the specified threshold and if the value is higher it is 
maintained along with
the spatial coordinates of the location in a list. We limit the maximum size of this list to around 1,000,000 locations in order to prevent having to return
the entire time-step or a significant fraction of it for queries with thresholds that are set too low. The list of locations and values are then maintained in a 
cache, which is local to the database node and indicates the function, time-step, spatial region and the threshold requested. We use a least recently
used cache replacement policy. All modifications of and queries to the cache are executed within a transaction with snapshot isolation level to
avoid dirty-reads or inconstant view of the cache.

The central part of the evaluation strategy for threshold queries that we have developed is the introduction of an application-aware cache
for query results. The results of these queries are small compared to the amount of data that need to be examined and the results can be used to
answer subsequent queries as long as they are within the same region and specify the same or higher threshold. Caching the query results
preserves the computational effort in addition to substantially reducing I/O. We do not have to derive the requested field from the raw data for 
queries that hit the cache. This results in a substantial improvement in query performance as we only have to scan
a small set of data and do not need to perform any additional computation.
\kk{Cite results for top-$k$ queries in databases and distributed systems. Remark that non of these strategies operate on large array datasets 
and they do not operate on derived fields, where each value of the derived field depends on multiple values from the original field and requires a kernel
computation accessing multiple data points in a kernel around the target location.}

The overall execution of threshold queries to the JHTDB is the following. The mediator submits a query to each of the database nodes storing the raw data
asynchronously. Each node begins evaluation of the query by first performing a cache lookup. If the data for the requested field, time-step and spatial region
are available in the cache and if the specified threshold is higher than the one stored in the cache the query can be answered from there. The records are
retrieved from the cache and the ones that have a higher value are returned to the mediator and subsequently back to the user. If the data stored in the
cache have a higher threshold than the one requested the cache needs to be updated. Similarly, if the cache does not have an entry for the specified 
parameters the query needs to be evaluated from the raw data. In that case the raw data are read into memory with data along the boundary requested from
adjacent nodes as needed. The specified field is derived at each location on the grid and the norm or absolute value of the field is compared against
the threshold. The locations where the values are higher than the threshold need to then be stored in the cache. If the cache does not have enough space
for the new records, space is freed up by removing the least recently used data.  Again, reading from, updating or modifying the cache is done within a
transaction with snapshot isolation level. Snapshot isolation allows us to avoid locking the tables that serve as the cache for each transaction. This
provides for a higher degree of parallelism and avoids any potential deadlocks from queries running in parallel. Figure \ref{figCacheLookup} illustrates this process.

\section{Experimental Results}
We evaluate the developed method for the execution of threshold queries to large numerical simulation datasets with the goal of analyzing the 
benefits and overhead from the introduction of the application-aware cache. We also analyze the scaling properties of the method. Finally, we show
that an integrated method that performs the evaluation on the database nodes near the data is several orders of magnitude faster than evaluating
a threshold locally by the user.

The experiments were run on the production database nodes of the JHTDB through a development Web-server hosting the Web-services. We used
the MHD dataset for the experimental runs. This dataset is partitioned across 4 database nodes according to spatial regions in the Morton z-order. 
The database nodes are 2.33 GHz dual quad-core Windows 2008 servers with SQL Server 2008 R2 and 24 GB of memory. Each node has 24 2TB 
SATA disks arranged as a set of four RAID-5 arrays. The database files are distributed in a round-robin fashion across the nodes and their associated disk
arrays. The tables storing the data are partitioned spatially along contiguous ranges of the Morton z-curve and the data for each partition reside in one
database file. 

For this evaluation we looked at the performance of threshold queries to the vorticity field. The vorticity field is representative of derived fields, which
have to be computed from the stored data. The vorticity is computed from the velocity field by taking its curl. As described in section \ref{science_use_cases}
thresholding the vorticity field is important in the study of fluid dynamics and clustering the locations of maximum vorticity can lead to new insights into
the development of the most intense vortices that are observed. \kk{Consider showing other fields as well; possibly thresholding the velocity by itself}

-- measure the overhead of querying the cache first:
Figure showing avg. execution times of queries for vorticity with a high threshold, medium threshold and low threshold. For each threshold value
plot the time to simply do the I/O, the time the query takes without the presence of a cache (the difference is essentially the time taken to compute
the vorticity field at each location on the grid) and the time the query takes with the inclusion of a cache. This shows that the overhead associated
with querying and updating the cache is not significant. Consider including a figure for a representative time-step showing the PDF of the vorticity field
to show how the vorticity values are distributed and to get an understanding for the different threshold levels. 

-- measure time taken to perform the thresholding computation for an entire time-step on a cold cache

-- measure time taken to evaluate the query after warming up the cache and polluting it afterward

-- provide scaling measurements

-- compare with local evaluation

\section{Related Work}
The processing of top-$k$ queries has been studied extensively in the context of distributed and relational database systems. A survey of different techniques
in the case of centralized processing is given in \cite{Ilyas}. In the case of distributed processing different approaches focus on horizontally 
\cite{Balke, Vlachou} or vertically \cite{Cao, Chaudhuri, Guntzer, Marian, Michel} distributed data. None of these approaches deals with array data stored in
a realtional database system. Moreover, these approaches focus on top-$k$ queries that use simple linear score functions to aggregate the atribute values of
individual records. In addition to top-$k$ queries we are also interested in threshold queries, where the result set includes all locations and values that are
above a user prescribed threshold. 

Zhao et al. propose an algorithm for the processing of top-$k$ queries in large-scale distributed environments called BRANCA \cite{Zhao}. They build on the 
idea of semantic caching \cite{Ren} and make use of brach caches, which store results of previous top-$k$ queries with respect to the data stored on each server.
The caching mecahnism that we use is similar in that regard, but the queries that are evaluated in our system operate on derived fields, which are computed
at each location by accessing data from a surrounding region. The queries described in \cite{Zhao} operate over the attributes of individual records only using
simple linear score functions.

A{\ss}falg et al. introduce the concept of threshold queries in time series databases \cite{Asfalg}. Their definition of threshold queries differs from the threshold
queries described in this paper. They are concerned with determining the time series, which exceed a user-defined threshold at time frames similar to
the time series specified in the query. Thus, their definition of threshold queries is concerned with the temporal relationship between the time series
stored in the database (usually one dimensional sequence of measurements) and the time series given in the query. In contrast, our approach focuses on
reporting all of the spatial locations of a multidimensional field where the norm or absolute value of the field exceeds a user prescribed threshold.

In a system called the tree cache Lopez et al. \cite{Lopez} make use of a small application-aware cache to reduce access time to large datasets stored on
disk. The tree cache stores individual octants of octree datasets and exploits application-specific information to determine which octants to cache and to 
perform query reordering. This work has inspired the use of an application-aware cache for the evaluation of threshold queries. In contrast to the tree cache
we do not cache raw data objects, but rather query results. This substantially reduces the size of the cache and improves query performance.

\section{Conclusions and Future Work}

%% \section{The {\secit Body} of The Paper}
%% Typically, the body of a paper is organized
%% into a hierarchical structure, with numbered or unnumbered
%% headings for sections, subsections, sub-subsections, and even
%% smaller sections.  The command \texttt{{\char'134}section} that
%% precedes this paragraph is part of such a
%% hierarchy.\footnote{This is the second footnote.  It
%% starts a series of three footnotes that add nothing
%% informational, but just give an idea of how footnotes work
%% and look. It is a wordy one, just so you see
%% how a longish one plays out.} \LaTeX\ handles the numbering
%% and placement of these headings for you, when you use
%% the appropriate heading commands around the titles
%% of the headings.  If you want a sub-subsection or
%% smaller part to be unnumbered in your output, simply append an
%% asterisk to the command name.  Examples of both
%% numbered and unnumbered headings will appear throughout the
%% balance of this sample document.

%% Because the entire article is contained in
%% the \textbf{document} environment, you can indicate the
%% start of a new paragraph with a blank line in your
%% input file; that is why this sentence forms a separate paragraph.

%% \subsection{Type Changes and {\subsecit Special} Characters}
%% We have already seen several typeface changes in this sample.  You
%% can indicate italicized words or phrases in your text with
%% the command \texttt{{\char'134}textit}; emboldening with the
%% command \texttt{{\char'134}textbf}
%% and typewriter-style (for instance, for computer code) with
%% \texttt{{\char'134}texttt}.  But remember, you do not
%% have to indicate typestyle changes when such changes are
%% part of the \textit{structural} elements of your
%% article; for instance, the heading of this subsection will
%% be in a sans serif\footnote{A third footnote, here.
%% Let's make this a rather short one to
%% see how it looks.} typeface, but that is handled by the
%% document class file. Take care with the use
%% of\footnote{A fourth, and last, footnote.}
%% the curly braces in typeface changes; they mark
%% the beginning and end of
%% the text that is to be in the different typeface.

%% You can use whatever symbols, accented characters, or
%% non-English characters you need anywhere in your document;
%% you can find a complete list of what is
%% available in the \textit{\LaTeX\
%% User's Guide}\cite{Lamport:LaTeX}.

%% \subsection{Math Equations}
%% You may want to display math equations in three distinct styles:
%% inline, numbered or non-numbered display.  Each of
%% the three are discussed in the next sections.

%% \subsubsection{Inline (In-text) Equations}
%% A formula that appears in the running text is called an
%% inline or in-text formula.  It is produced by the
%% \textbf{math} environment, which can be
%% invoked with the usual \texttt{{\char'134}begin. . .{\char'134}end}
%% construction or with the short form \texttt{\$. . .\$}. You
%% can use any of the symbols and structures,
%% from $\alpha$ to $\omega$, available in
%% \LaTeX\cite{Lamport:LaTeX}; this section will simply show a
%% few examples of in-text equations in context. Notice how
%% this equation: \begin{math}\lim_{n\rightarrow \infty}x=0\end{math},
%% set here in in-line math style, looks slightly different when
%% set in display style.  (See next section).

%% \subsubsection{Display Equations}
%% A numbered display equation -- one set off by vertical space
%% from the text and centered horizontally -- is produced
%% by the \textbf{equation} environment. An unnumbered display
%% equation is produced by the \textbf{displaymath} environment.

%% Again, in either environment, you can use any of the symbols
%% and structures available in \LaTeX; this section will just
%% give a couple of examples of display equations in context.
%% First, consider the equation, shown as an inline equation above:
%% \begin{equation}\lim_{n\rightarrow \infty}x=0\end{equation}
%% Notice how it is formatted somewhat differently in
%% the \textbf{displaymath}
%% environment.  Now, we'll enter an unnumbered equation:
%% \begin{displaymath}\sum_{i=0}^{\infty} x + 1\end{displaymath}
%% and follow it with another numbered equation:
%% \begin{equation}\sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f\end{equation}
%% just to demonstrate \LaTeX's able handling of numbering.

%% \subsection{Citations}
%% Citations to articles \cite{bowman:reasoning, clark:pct, braams:babel, herlihy:methodology},
%% conference
%% proceedings \cite{clark:pct} or books \cite{salas:calculus, Lamport:LaTeX} listed
%% in the Bibliography section of your
%% article will occur throughout the text of your article.
%% You should use BibTeX to automatically produce this bibliography;
%% you simply need to insert one of several citation commands with
%% a key of the item cited in the proper location in
%% the \texttt{.tex} file \cite{Lamport:LaTeX}.
%% The key is a short reference you invent to uniquely
%% identify each work; in this sample document, the key is
%% the first author's surname and a
%% word from the title.  This identifying key is included
%% with each item in the \texttt{.bib} file for your article.

%% The details of the construction of the \texttt{.bib} file
%% are beyond the scope of this sample document, but more
%% information can be found in the \textit{Author's Guide},
%% and exhaustive details in the \textit{\LaTeX\ User's
%% Guide}\cite{Lamport:LaTeX}.

%% This article shows only the plainest form
%% of the citation command, using \texttt{{\char'134}cite}.
%% This is what is stipulated in the SIGS style specifications.
%% No other citation format is endorsed.

%% \subsection{Tables}
%% Because tables cannot be split across pages, the best
%% placement for them is typically the top of the page
%% nearest their initial cite.  To
%% ensure this proper ``floating'' placement of tables, use the
%% environment \textbf{table} to enclose the table's contents and
%% the table caption.  The contents of the table itself must go
%% in the \textbf{tabular} environment, to
%% be aligned properly in rows and columns, with the desired
%% horizontal and vertical rules.  Again, detailed instructions
%% on \textbf{tabular} material
%% is found in the \textit{\LaTeX\ User's Guide}.

%% Immediately following this sentence is the point at which
%% Table 1 is included in the input file; compare the
%% placement of the table here with the table in the printed
%% dvi output of this document.

%% \begin{table}
%% \centering
%% \caption{Frequency of Special Characters}
%% \begin{tabular}{|c|c|l|} \hline
%% Non-English or Math&Frequency&Comments\\ \hline
%% \O & 1 in 1,000& For Swedish names\\ \hline
%% $\pi$ & 1 in 5& Common in math\\ \hline
%% \$ & 4 in 5 & Used in business\\ \hline
%% $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%% \hline\end{tabular}
%% \end{table}

%% To set a wider table, which takes up the whole width of
%% the page's live area, use the environment
%% \textbf{table*} to enclose the table's contents and
%% the table caption.  As with a single-column table, this wide
%% table will ``float" to a location deemed more desirable.
%% Immediately following this sentence is the point at which
%% Table 2 is included in the input file; again, it is
%% instructive to compare the placement of the
%% table here with the table in the printed dvi
%% output of this document.

%% \begin{table*}
%% \centering
%% \caption{Some Typical Commands}
%% \begin{tabular}{|c|c|l|} \hline
%% Command&A Number&Comments\\ \hline
%% \texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
%% \texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
%% \texttt{{\char'134}table}& 300 & For tables\\ \hline
%% \texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
%% \end{table*}
%% % end the environment with {table*}, NOTE not {table}!

%% \subsection{Figures}
%% Like tables, figures cannot be split across pages; the
%% best placement for them
%% is typically the top or the bottom of the page nearest
%% their initial cite.  To ensure this proper ``floating'' placement
%% of figures, use the environment
%% \textbf{figure} to enclose the figure and its caption.

%% This sample document contains examples of \textbf{.eps}
%% and \textbf{.ps} files to be displayable with \LaTeX.  More
%% details on each of these is found in the \textit{Author's Guide}.

%% \begin{figure}
%% \centering
%% \epsfig{file=fly.eps}
%% \caption{A sample black and white graphic (.eps format).}
%% \end{figure}

%% \begin{figure}
%% \centering
%% \epsfig{file=fly.eps, height=1in, width=1in}
%% \caption{A sample black and white graphic (.eps format)
%% that has been resized with the \texttt{epsfig} command.}
%% \end{figure}

%% As was the case with tables, you may want a figure
%% that spans two columns.  To do this, and still to
%% ensure proper ``floating'' placement of tables, use the environment
%% \textbf{figure*} to enclose the figure and its caption.

%% Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
%% used; use
%% the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
%% commands as appropriate for the different file types.

%% \subsection{Theorem-like Constructs}
%% Other common constructs that may occur in your article are
%% the forms for logical constructs like theorems, axioms,
%% corollaries and proofs.  There are
%% two forms, one produced by the
%% command \texttt{{\char'134}newtheorem} and the
%% other by the command \texttt{{\char'134}newdef}; perhaps
%% the clearest and easiest way to distinguish them is
%% to compare the two in the output of this sample document:

%% This uses the \textbf{theorem} environment, created by
%% the\linebreak\texttt{{\char'134}newtheorem} command:
%% \newtheorem{theorem}{Theorem}
%% \begin{theorem}
%% Let $f$ be continuous on $[a,b]$.  If $G$ is
%% an antiderivative for $f$ on $[a,b]$, then
%% \begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
%% \end{theorem}

%% The other uses the \textbf{definition} environment, created
%% by the \texttt{{\char'134}newdef} command:
%% \newdef{definition}{Definition}
%% \begin{definition}
%% If $z$ is irrational, then by $e^z$ we mean the
%% unique number which has
%% logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
%% \end{definition}

%% \begin{figure}
%% \centering
%% \psfig{file=rosette.ps, height=1in, width=1in,}
%% \caption{A sample black and white graphic (.ps format) that has
%% been resized with the \texttt{psfig} command.}
%% \end{figure}

%% Two lists of constructs that use one of these
%% forms is given in the
%% \textit{Author's  Guidelines}.

%% \begin{figure*}
%% \centering
%% \epsfig{file=flies.eps}
%% \caption{A sample black and white graphic (.eps format)
%% that needs to span two columns of text.}
%% \end{figure*}
%% and don't forget to end the environment with
%% {figure*}, not {figure}!
 
%% There is one other similar construct environment, which is
%% already set up
%% for you; i.e. you must \textit{not} use
%% a \texttt{{\char'134}newdef} command to
%% create it: the \textbf{proof} environment.  Here
%% is a example of its use:
%% \begin{proof}
%% Suppose on the contrary there exists a real number $L$ such that
%% \begin{displaymath}
%% \lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
%% \end{displaymath}
%% Then
%% \begin{displaymath}
%% l=\lim_{x\rightarrow c} f(x)
%% = \lim_{x\rightarrow c}
%% \left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
%% = \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
%% \frac{f(x)}{g(x)} = 0\cdot L = 0,
%% \end{displaymath}
%% which contradicts our assumption that $l\neq 0$.
%% \end{proof}

%% Complete rules about using these environments and using the
%% two different creation commands are in the
%% \textit{Author's Guide}; please consult it for more
%% detailed instructions.  If you need to use another construct,
%% not listed therein, which you want to have the same
%% formatting as the Theorem
%% or the Definition\cite{salas:calculus} shown above,
%% use the \texttt{{\char'134}newtheorem} or the
%% \texttt{{\char'134}newdef} command,
%% respectively, to create it.

%% \subsection*{A {\secit Caveat} for the \TeX\ Expert}
%% Because you have just been given permission to
%% use the \texttt{{\char'134}newdef} command to create a
%% new form, you might think you can
%% use \TeX's \texttt{{\char'134}def} to create a
%% new command: \textit{Please refrain from doing this!}
%% Remember that your \LaTeX\ source code is primarily intended
%% to create camera-ready copy, but may be converted
%% to other forms -- e.g. HTML. If you inadvertently omit
%% some or all of the \texttt{{\char'134}def}s recompilation will
%% be, to say the least, problematic.

%% \section{Conclusions}
%% This paragraph will end the body of this sample document.
%% Remember that you might still have Acknowledgments or
%% Appendices; brief samples of these
%% follow.  There is still the Bibliography to deal with; and
%% we will make a disclaimer about that here: with the exception
%% of the reference to the \LaTeX\ book, the citations in
%% this paper are to articles which have nothing to
%% do with the present subject and are used as
%% examples only.
%% %\end{document}  % This is where a 'short' article might terminate

\section{Acknowledgment}

%% %ACKNOWLEDGMENTS are optional
%% \section{Acknowledgments}
%% This section is optional; it is a location for you
%% to acknowledge grants, funding, editing assistance and
%% what have you.  In the present case, for example, the
%% authors would like to thank Gerald Murray of ACM for
%% his help in codifying this \textit{Author's Guide}
%% and the \textbf{.cls} and \textbf{.tex} files that it describes.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%% \appendix
%% %Appendix A
%% \section{Headings in Appendices}
%% The rules about hierarchical headings discussed above for
%% the body of the article are different in the appendices.
%% In the \textbf{appendix} environment, the command
%% \textbf{section} is used to
%% indicate the start of each Appendix, with alphabetic order
%% designation (i.e. the first is A, the second B, etc.) and
%% a title (if you include one).  So, if you need
%% hierarchical structure
%% \textit{within} an Appendix, start with \textbf{subsection} as the
%% highest level. Here is an outline of the body of this
%% document in Appendix-appropriate form:
%% \subsection{Introduction}
%% \subsection{The Body of the Paper}
%% \subsubsection{Type Changes and  Special Characters}
%% \subsubsection{Math Equations}
%% \paragraph{Inline (In-text) Equations}
%% \paragraph{Display Equations}
%% \subsubsection{Citations}
%% \subsubsection{Tables}
%% \subsubsection{Figures}
%% \subsubsection{Theorem-like Constructs}
%% \subsubsection*{A Caveat for the \TeX\ Expert}
%% \subsection{Conclusions}
%% \subsection{Acknowledgments}
%% \subsection{Additional Authors}
%% This section is inserted by \LaTeX; you do not insert it.
%% You just add the names and information in the
%% \texttt{{\char'134}additionalauthors} command at the start
%% of the document.
%% \subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.
%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The acm\_proc\_article-sp document class file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.
%% \balancecolumns
%% % That's all folks!
\end{document}
